adapt:
  audio_emb_size: 300
  nb_layers: 1
data:
  root_dir: data
  embedding: clap
  max_audio_len: 2048
  max_caption_tok_len: 64
lm:
  audio_enc_path: clap_audio_encoder.pth
  freeze_audio_enc: true
  config: # Model parameters
    activation_dropout: 0.1
    activation_function: 'gelu'
    attention_dropout: 0.1
    classifier_dropout: 0.0
    d_model: 768
    decoder_attention_heads: 12
    decoder_ffn_dim: 3072
    decoder_layers: 6
    dropout: 0.1
    encoder_attention_heads: 12
    encoder_ffn_dim: 3072
    encoder_layers: 0
    vocab_size: 50265
  generation: # Generation parameters
    early_stopping: true
    no_repeat_ngram_size: 3
    num_beams: 4
    min_length: 5
    max_length: 100
    length_penalty: 1.0
    decoding: beam
  eval_model: /home/ubuntu/s3-mount/audio_captioning/baseline/dcase-2023-baseline/outputs/clap_exp_out/pytorch_model_best.bin
  eval_checkpoint: 0
  freeze:
    all: false
    attn: false
    dec: false
    dec_attn: false
    dec_mlp: false
    dec_self_attn: false
    enc: false
    enc_attn: false
    enc_mlp: false
    mlp: false
  tokenizer: facebook/bart-base
  pretrained: null
training:
  eval_steps: 1000
  force_cpu: false
  batch_size: 4
  gradient_accumulation_steps: 2
  num_workers: 8
  lr: 1.0e-05
  nb_epochs: 20
  save_steps: 1000
  seed: 0
workflow:
  train: false
  validate: false
  evaluate: true
  infer: false
