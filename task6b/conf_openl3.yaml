trial_series: /home/ubuntu/dcase2023-audio-retrieval/output/

trial_base: openl3_cnn




# Configure training, validation, and evaluation data
data_conf:
    train_data: # training data
        dataset: /home/ubuntu/audio-captioning-and-audio-retrieval/openl3_model/data/data/Clotho
        audio_data: development_audio_logmels_openl3.hdf5
        text_data: development_text.csv
        text_embeds: sbert_embeds.pkl
        text_level: sentence

    val_data: # validation data
        dataset: /home/ubuntu/audio-captioning-and-audio-retrieval/openl3_model/data/data/Clotho
        audio_data: validation_audio_logmels_openl3.hdf5
        text_data: validation_text.csv
        text_embeds: sbert_embeds.pkl
        text_level: sentence

    eval_data: # evaluation data
        dataset: /home/ubuntu/audio-captioning-and-audio-retrieval/openl3_model/data/data/Clotho
        audio_data: evaluation_audio_logmels_openl3.hdf5
        text_data: evaluation_text.csv
        text_embeds: sbert_embeds.pkl
        text_level: sentence


# Configure hyper-parameters
param_conf:
    num_epoch: 30
    batch_size: 32
    model: DualEncoderModel
    criterion: infonce_loss
    optimizer: AdamOptimizer
    lr_scheduler: ReduceLROnPlateau


# Model definitions
DualEncoderModel:
    name: DualEncoderModel
    out_norm: L2

    audio_enc:
        name: OpenL3Embeddings
        init: nonprior
        weight: /home/ubuntu/dcase2023-audio-retrieval/data/pretrained_models/CNN14_300.pth
        trainable: true
        out_dim: 300
        freeze_audio_enc: true
        
        activation_dropout: 0.1
        activation_function: 'gelu'
        attention_dropout: 0.1
        classifier_dropout: 0.0
        d_model: 300
        decoder_attention_heads: 12
        decoder_ffn_dim: 3072
        decoder_layers: 6
        dropout: 0.1
        encoder_attention_heads: 12
        audio_emb_size: 512
        nb_layers: 1
        encoder_ffn_dim: 3072
        encoder_layers: 0
        vocab_size: 50265
        early_stopping: true
        no_repeat_ngram_size: 3
        num_beams: 4
        min_length: 5
        max_length: 100
        length_penalty: 1.0
        decoding: beam
        eval_model: best
        eval_checkpoint: 0
        freeze_all: false
        freeze_attn: false
        freeze_dec: false
        freeze_dec_attn: false
        freeze_dec_mlp: false
        freeze_dec_self_attn: false
        freeze_enc: false
        freeze_enc_attn: false
        freeze_enc_mlp: false
        freeze_mlp: false
        tokenizer: facebook/bart-base
        pretrained: null
        training:
        eval_steps: 1000
        force_cpu: false
        batch_size: 4
        gradient_accumulation_steps: 2
        num_workers: 8
        lr: 1.0e-05
        nb_epochs: 20
        save_steps: 1000
        seed: 0

    text_enc:
        name: SentBERTBaseEncoder
        init: prior
        out_dim: 300


# Criteria
criteria:
    infonce_loss:
        name: LogSoftmaxLoss
        args:
            temperature: 0.07
            dist: dot_product  # dot_product, cosine_similarity


# Optimizer definitions
AdamOptimizer:
    name: Adam
    args:
        lr: 0.001
        weight_decay: 0.0


# Learning rate scheduler definitions
ReduceLROnPlateau:
    name: ReduceLROnPlateau
    args:
        mode: min
        factor: 0.1
        patience: 5
        threshold: 0.005
        threshold_mode: abs
        min_lr: 0.000001
        verbose: true
